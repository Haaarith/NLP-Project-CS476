{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "id": "A_shXSlYBmzf"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c52144478e304f76ce5310ab30a72a08e0c35dd",
    "id": "8zem8hYnBmzf"
   },
   "source": [
    "We first start by loading the raw data for Hotel reviews and Amazon reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "cb56534fa07076079598d82a2e69848802148d70",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "eEzwb9iwBmzg",
    "outputId": "1619ded0-e5f3-4b8f-b813-8f5e90d3bc43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hotel Reviews\n",
      "                                              review  is_neagtive_review\n",
      "0   I am so angry that i made this post available...                   1\n",
      "1  No Negative No real complaints the hotel was g...                   0\n",
      "2   Rooms are nice but for elderly a bit difficul...                   0\n",
      "3   My room was dirty and I was afraid to walk ba...                   1\n",
      "4   You When I booked with your company on line y...                   0\n",
      "\n",
      "\n",
      "Amazon Reviews\n",
      "                                              Review  is_neagtive_review\n",
      "0  I have bought several of the Vitality canned d...                   1\n",
      "1  Product arrived labeled as Jumbo Salted Peanut...                   0\n",
      "2  This is a confection that has been around a fe...                   1\n",
      "3  If you are looking for the secret ingredient i...                   0\n",
      "4  Great taffy at a great price.  There was a wid...                   1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read data Amazon reviews and hotel reviews\n",
    "Amazonreviews_df = pd.read_csv(\"AmazonReviews.csv\")\n",
    "reviews_df = pd.read_csv(\"Hotel_Reviews.csv\")\n",
    "\n",
    "# append the positive and negative text reviews\n",
    "reviews_df[\"review\"] = reviews_df[\"Negative_Review\"] + reviews_df[\"Positive_Review\"]\n",
    "\n",
    "# create the label\n",
    "reviews_df[\"is_neagtive_review\"] = reviews_df[\"Reviewer_Score\"].apply(lambda x: 1 if x < 5 else 0)\n",
    "\n",
    "# select only relevant columns for Hotel reviews\n",
    "reviews_df = reviews_df[[\"review\", \"is_neagtive_review\"]]\n",
    "\n",
    "# select only relevant columns for Amazon reviews\n",
    "Amazonreviews_df = pd.DataFrame(Amazonreviews_df,columns = ['Score','Text'])\n",
    "Amazonreviews_df.rename(columns = {'Score':'Rating','Text':'Review'},inplace = True)\n",
    "\n",
    "#function to do sentiment for amazon reviews if the review rating is\n",
    "# less or equal to 3 it is neagtive, if it bigger than 3 it is positive\n",
    "\n",
    "def sentimnent_for_amazon(rating):\n",
    "    if(rating<=3):\n",
    "        return 0\n",
    "    else :\n",
    "        return 1\n",
    "\n",
    "#appllying the function on the rating colomun\n",
    "Amazonreviews_df['is_neagtive_review'] = Amazonreviews_df['Rating'].apply(sentimnent_for_amazon)\n",
    "Amazonreviews_df.drop(['Rating'],axis = 1, inplace = True)\n",
    "print('Hotel Reviews')\n",
    "print(reviews_df.head())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Amazon Reviews')\n",
    "print(Amazonreviews_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ORVQa4voBmzh"
   },
   "outputs": [],
   "source": [
    "reviews_df['predicted_review']=reviews_df.apply(lambda x: 1 if 'angry' in x['review'] else 0, axis=1)\n",
    "Amazonreviews_df['predicted_review']=Amazonreviews_df.apply(lambda x: 1 if 'disappointed' in x['Review'] else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YaQNfz-pBmzi",
    "outputId": "1226a593-1631-4b09-ead4-e5f74471d114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(493387, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(122689, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(reviews_df[reviews_df['predicted_review']==reviews_df['is_neagtive_review']].shape)\n",
    "Amazonreviews_df[Amazonreviews_df['predicted_review']==Amazonreviews_df['is_neagtive_review']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZbAoJWOvBmzj",
    "outputId": "d5b34cf2-c34e-4328-9557-be4909ed5cc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568454, 3)\n",
      "(515738, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(Amazonreviews_df.shape)\n",
    "print(reviews_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GH1WKATrBmzk",
    "outputId": "f1d4ec42-1004-4799-c396-22a43f38e4bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9566836002636988"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "49340/51574"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PpYjQKbIBmzl",
    "outputId": "bed27571-0ed9-44c7-99b3-2f96c2e38202"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hotel Reviews\n",
      "0    493457\n",
      "1     22281\n",
      "Name: is_neagtive_review, dtype: int64\n",
      "\n",
      "\n",
      "Amazon Reviews\n",
      "1    443777\n",
      "0    124677\n",
      "Name: is_neagtive_review, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Hotel Reviews')\n",
    "print(reviews_df['is_neagtive_review'].value_counts())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Amazon Reviews')\n",
    "print(Amazonreviews_df['is_neagtive_review'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NL6DRdSiBmzl",
    "outputId": "54ea401e-2fc6-467e-9d7a-1e40a408b1eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hotel Reviews positive and negative ratio\n",
      "0    0.956798\n",
      "1    0.043202\n",
      "Name: is_neagtive_review, dtype: float64\n",
      "\n",
      "\n",
      "Amazon Reviews positive and negative ratio\n",
      "1    0.780674\n",
      "0    0.219326\n",
      "Name: is_neagtive_review, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Hotel Reviews positive and negative ratio')\n",
    "print(reviews_df['is_neagtive_review'].value_counts(normalize=True))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Amazon Reviews positive and negative ratio')\n",
    "print(Amazonreviews_df['is_neagtive_review'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "HJYEbMsyBmzm"
   },
   "outputs": [],
   "source": [
    "reviews_df['new_stupid_model']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GL3bMfLuBmzm",
    "outputId": "ca894ad3-9bd2-4cd4-c0ba-30e953f6dc9d"
   },
   "outputs": [],
   "source": [
    "reviews_df[reviews_df['new_stupid_model']==reviews_df['is_neagtive_review']].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "137871905d305dae06cbc7c9f263b3ac3a6b85b7",
    "id": "f-OeocjjBmzn"
   },
   "source": [
    "# Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "a4f04ca1ef2a8b8b98bdbba2a3563b776a583964",
    "id": "IlWET-iQBmzn"
   },
   "outputs": [],
   "source": [
    "reviews_df = reviews_df.sample(frac = 0.1, replace = False, random_state=42)\n",
    "Amazonreviews_df = Amazonreviews_df.sample(frac = 0.1, replace = False, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f57e5b9e22b0ac6d9a1fb98224da2d2417e991f4",
    "id": "4i6kY5thBmzo"
   },
   "source": [
    "Reviews data is sampled in order to speed up computations because data is very large around 500k entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0f2061b96461095869fe3cbd19ae10ca9c651482",
    "id": "y70rWxwMBmzo"
   },
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "256f6d920bcb8b50725cb93488a9e7be4c772349",
    "id": "1HMNpAKFBmzp"
   },
   "source": [
    "If the user doesn't leave any negative feedback comment, this will appear as \"No Negative\" in our data. This is the same for the positive comments with the default value \"No Positive\". We have to remove those parts from our texts.\n",
    "\n",
    "The next step consists in cleaning the text data with various operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "005e87154e11f530b58c56cac9f555c82493b906",
    "id": "w2dUJ_u8Bmzo"
   },
   "outputs": [],
   "source": [
    "# remove 'No Negative' or 'No Positive' from text\n",
    "reviews_df[\"review\"] = reviews_df[\"review\"].apply(lambda x: x.replace(\"No Negative\", \"\").replace(\"No Positive\", \"\"))\n",
    "Amazonreviews_df[\"Review\"] = Amazonreviews_df[\"Review\"].apply(lambda x: x.replace(\"No Negative\", \"\").replace(\"No Positive\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XA0fUkRCKcOO",
    "outputId": "9c589ea5-3276-4293-811b-74e071ac4f8a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Harith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Harith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Harith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Harith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ab1d87cea172f979af13ba58cafed1302267b183",
    "id": "QRdvmDvnBmzq"
   },
   "source": [
    "To clean textual data, we call our custom 'clean_text' function that performs several transformations:\n",
    "- lower the text\n",
    "- tokenize the text (split the text into words) and remove the punctuation\n",
    "- remove useless words that contain numbers\n",
    "- remove useless stop words like 'the', 'a' ,'this' etc.\n",
    "- Part-Of-Speech (POS) tagging: assign a tag to every word to define if it corresponds to a noun, a verb etc. using the WordNet lexical database\n",
    "- lemmatize the text: transform every word into their root form (e.g. rooms -> room, slept -> sleep)\n",
    "\n",
    "Now that we have cleaned our data, we can do some feature engineering for our modelization part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "2d51a84a81d55c527dbd56b42201f5df3d444ce5",
    "id": "HegrHQhQBmzp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Harith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Harith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# return the wordnet object value corresponding to the POS tag\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "import string\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def clean_text(text):\n",
    "    # lower text\n",
    "    text = text.lower()\n",
    "    # tokenize text and remove puncutation\n",
    "    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
    "    # remove words that contain numbers\n",
    "    text = [word for word in text if not any(c.isdigit() for c in word)]\n",
    "    # remove stop words\n",
    "    stop = stopwords.words('english')\n",
    "    text = [x for x in text if x not in stop]\n",
    "    # remove empty tokens\n",
    "    text = [t for t in text if len(t) > 0]\n",
    "    # pos tag text\n",
    "    pos_tags = pos_tag(text)\n",
    "    # lemmatize text\n",
    "    text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n",
    "    # remove words with only one letter\n",
    "    text = [t for t in text if len(t) > 1]\n",
    "    # join all\n",
    "    text = \" \".join(text)\n",
    "    return(text)\n",
    "\n",
    "# clean text data\n",
    "reviews_df[\"review_clean\"] = reviews_df[\"review\"].apply(lambda x: clean_text(x))\n",
    "Amazonreviews_df[\"review_clean\"] = Amazonreviews_df[\"Review\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "79ed7c7647f7868d74bb5b3267a77914539dd61d",
    "id": "WBIMHeiGBmzq"
   },
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0c9161a58d443d38ca818ef7deb128cc92359b5c",
    "id": "PwIVEs5RBmzq"
   },
   "source": [
    "We first start by adding sentiment analysis features because we can guess that customers reviews are highly linked to how they felt about their stay at the hotel. We use Vader, which is a part of the NLTK module designed for sentiment analysis. Vader uses a lexicon of words to find which ones are positives or negatives. It also takes into accout the context of the sentences to determine the sentiment scores. For each text, Vader retuns 4 values:\n",
    "- a neutrality score\n",
    "- a positivity score\n",
    "- a negativity score\n",
    "- an overall score that summarizes the previous scores\n",
    "\n",
    "We will integrate those 4 values as features in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "edbc356b3b7289c9aaafeafb41c11eb7c5a601f7",
    "id": "86XZ9Mr7Bmzq",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# add sentiment anaylsis columns\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "reviews_df[\"sentiments\"] = reviews_df[\"review\"].apply(lambda x: sid.polarity_scores(x))\n",
    "reviews_df = pd.concat([reviews_df.drop(['sentiments'], axis=1), reviews_df['sentiments'].apply(pd.Series)], axis=1)\n",
    "\n",
    "Amazonreviews_df[\"sentiments\"] = Amazonreviews_df[\"Review\"].apply(lambda x: sid.polarity_scores(x))\n",
    "Amazonreviews_df = pd.concat([Amazonreviews_df.drop(['sentiments'], axis=1), Amazonreviews_df['sentiments'].apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "892e8afaecce1fbfcbd61cfc5cbfea2a122c0296",
    "id": "nFqxcD7mBmzr"
   },
   "source": [
    "Next, we add some simple metrics for every text:\n",
    "- number of characters in the text\n",
    "- number of words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "183c8e5ad5659fea721bed9cdf40b45757f49721",
    "id": "PdoCwzo1Bmzr",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# add number of characters column\n",
    "reviews_df[\"nb_chars\"] = reviews_df[\"review\"].apply(lambda x: len(x))\n",
    "Amazonreviews_df[\"nb_chars\"] = Amazonreviews_df[\"Review\"].apply(lambda x: len(x))\n",
    "\n",
    "# add number of words column\n",
    "reviews_df[\"nb_words\"] = reviews_df[\"review\"].apply(lambda x: len(x.split(\" \")))\n",
    "Amazonreviews_df[\"nb_words\"] = Amazonreviews_df[\"Review\"].apply(lambda x: len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "905b125a775fa13b20d869443dc439d324a124b7",
    "id": "L-CaojFgBmzr"
   },
   "outputs": [],
   "source": [
    "#Hotel Reviews\n",
    "# create doc2vec vector columns\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "Hotel_documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(reviews_df[\"review_clean\"].apply(lambda x: x.split(\" \")))]\n",
    "\n",
    "# train a Doc2Vec model with our text data\n",
    "model = Doc2Vec(Hotel_documents, vector_size=5, window=2, min_count=1, workers=4)\n",
    "\n",
    "# transform each document into a vector data\n",
    "Hotel_doc2vec_df = reviews_df[\"review_clean\"].apply(lambda x: model.infer_vector(x.split(\" \"))).apply(pd.Series)\n",
    "Hotel_doc2vec_df.columns = [\"doc2vec_vector_\" + str(x) for x in Hotel_doc2vec_df.columns]\n",
    "reviews_df = pd.concat([reviews_df, Hotel_doc2vec_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0b1c2be5745ae8ecf348212b5af359f3c9ce09c1",
    "id": "p41jzugtBmzs"
   },
   "source": [
    "The next step consist in extracting vector representations for every review. The module Gensim creates a numerical vector representation of every word in the corpus by using the contexts in which they appear (Word2Vec). This is performed using shallow neural networks. What's interesting is that similar words will have similar representation vectors.\n",
    "\n",
    "Each text can also be transformed into numerical vectors using the word vectors (Doc2Vec). Same texts will also have similar representations and that is why we can use those vectors as training features.\n",
    "\n",
    "We first have to train a Doc2Vec model by feeding in our text data. By applying this model on our reviews, we can get those representation vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amazon Reviews \n",
    "# create doc2vec vector columns\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "Amazon_documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(Amazonreviews_df[\"review_clean\"].apply(lambda x: x.split(\" \")))]\n",
    "\n",
    "# train a Doc2Vec model with our text data\n",
    "model = Doc2Vec(Amazon_documents, vector_size=5, window=2, min_count=1, workers=4)\n",
    "\n",
    "# transform each document into a vector data\n",
    "Amazon_doc2vec_df = Amazonreviews_df[\"review_clean\"].apply(lambda x: model.infer_vector(x.split(\" \"))).apply(pd.Series)\n",
    "Amazon_doc2vec_df.columns = [\"doc2vec_vector_\" + str(x) for x in Amazon_doc2vec_df.columns]\n",
    "Amazonreviews_df = pd.concat([Amazonreviews_df, Amazon_doc2vec_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>is_neagtive_review</th>\n",
       "      <th>predicted_review</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>nb_chars</th>\n",
       "      <th>nb_words</th>\n",
       "      <th>doc2vec_vector_0</th>\n",
       "      <th>doc2vec_vector_1</th>\n",
       "      <th>doc2vec_vector_2</th>\n",
       "      <th>doc2vec_vector_3</th>\n",
       "      <th>doc2vec_vector_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>488440</th>\n",
       "      <td>Would have appreciated a shop in the hotel th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>would appreciate shop hotel sell drinking wate...</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.9924</td>\n",
       "      <td>599</td>\n",
       "      <td>113</td>\n",
       "      <td>-0.197011</td>\n",
       "      <td>0.223198</td>\n",
       "      <td>-0.294666</td>\n",
       "      <td>0.063046</td>\n",
       "      <td>-0.393775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274649</th>\n",
       "      <td>No tissue paper box was present at the room</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>tissue paper box present room</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>44</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.038253</td>\n",
       "      <td>0.151030</td>\n",
       "      <td>0.091067</td>\n",
       "      <td>-0.079733</td>\n",
       "      <td>-0.012620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374688</th>\n",
       "      <td>Pillows  Nice welcoming and service</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pillow nice welcome service</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.6908</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0.024773</td>\n",
       "      <td>0.067819</td>\n",
       "      <td>0.078592</td>\n",
       "      <td>-0.100598</td>\n",
       "      <td>0.012816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404352</th>\n",
       "      <td>Everything including the nice upgrade The Hot...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>everything include nice upgrade hotel revamp s...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.621</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.9153</td>\n",
       "      <td>155</td>\n",
       "      <td>27</td>\n",
       "      <td>0.049246</td>\n",
       "      <td>0.202737</td>\n",
       "      <td>-0.032925</td>\n",
       "      <td>-0.078608</td>\n",
       "      <td>0.031952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451596</th>\n",
       "      <td>Lovely hotel v welcoming staff</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>lovely hotel welcome staff</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.7717</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.064352</td>\n",
       "      <td>0.148502</td>\n",
       "      <td>0.015996</td>\n",
       "      <td>-0.077464</td>\n",
       "      <td>-0.042423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review  is_neagtive_review  \\\n",
       "488440   Would have appreciated a shop in the hotel th...                   0   \n",
       "274649        No tissue paper box was present at the room                   0   \n",
       "374688                Pillows  Nice welcoming and service                   0   \n",
       "404352   Everything including the nice upgrade The Hot...                   0   \n",
       "451596                    Lovely hotel v welcoming staff                    0   \n",
       "\n",
       "        predicted_review                                       review_clean  \\\n",
       "488440                 0  would appreciate shop hotel sell drinking wate...   \n",
       "274649                 0                      tissue paper box present room   \n",
       "374688                 0                        pillow nice welcome service   \n",
       "404352                 0  everything include nice upgrade hotel revamp s...   \n",
       "451596                 0                         lovely hotel welcome staff   \n",
       "\n",
       "          neg    neu    pos  compound  nb_chars  nb_words  doc2vec_vector_0  \\\n",
       "488440  0.049  0.617  0.334    0.9924       599       113         -0.197011   \n",
       "274649  0.216  0.784  0.000   -0.2960        44        10         -0.038253   \n",
       "374688  0.000  0.345  0.655    0.6908        36         7          0.024773   \n",
       "404352  0.000  0.621  0.379    0.9153       155        27          0.049246   \n",
       "451596  0.000  0.230  0.770    0.7717        32         7         -0.064352   \n",
       "\n",
       "        doc2vec_vector_1  doc2vec_vector_2  doc2vec_vector_3  doc2vec_vector_4  \n",
       "488440          0.223198         -0.294666          0.063046         -0.393775  \n",
       "274649          0.151030          0.091067         -0.079733         -0.012620  \n",
       "374688          0.067819          0.078592         -0.100598          0.012816  \n",
       "404352          0.202737         -0.032925         -0.078608          0.031952  \n",
       "451596          0.148502          0.015996         -0.077464         -0.042423  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "_uuid": "6682eda4d0078a4953f5111717e4750c8f98d409",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "id": "2kX2M7UxBmzs",
    "outputId": "28cbd917-d119-4daf-e15a-3fffe87d039f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>is_neagtive_review</th>\n",
       "      <th>predicted_review</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>nb_chars</th>\n",
       "      <th>nb_words</th>\n",
       "      <th>doc2vec_vector_0</th>\n",
       "      <th>doc2vec_vector_1</th>\n",
       "      <th>doc2vec_vector_2</th>\n",
       "      <th>doc2vec_vector_3</th>\n",
       "      <th>doc2vec_vector_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165256</th>\n",
       "      <td>Having tried a couple of other brands of glute...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>tried couple brand gluten-free sandwich cooky ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.9684</td>\n",
       "      <td>485</td>\n",
       "      <td>87</td>\n",
       "      <td>0.255631</td>\n",
       "      <td>0.543345</td>\n",
       "      <td>0.316511</td>\n",
       "      <td>0.141402</td>\n",
       "      <td>-0.031420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231465</th>\n",
       "      <td>My cat loves these treats. If ever I can't fin...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>cat love treat ever can't find house pop top b...</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.7920</td>\n",
       "      <td>490</td>\n",
       "      <td>99</td>\n",
       "      <td>-0.078403</td>\n",
       "      <td>0.205094</td>\n",
       "      <td>0.073949</td>\n",
       "      <td>-0.509898</td>\n",
       "      <td>-0.354058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427827</th>\n",
       "      <td>A little less than I expected.  It tends to ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>little less expected tends muddy taste expect ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>136</td>\n",
       "      <td>29</td>\n",
       "      <td>0.120996</td>\n",
       "      <td>0.146771</td>\n",
       "      <td>0.109834</td>\n",
       "      <td>-0.118422</td>\n",
       "      <td>0.041968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433954</th>\n",
       "      <td>First there was Frosted Mini-Wheats, in origin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>first frost mini-wheats original size frost mi...</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.9923</td>\n",
       "      <td>1631</td>\n",
       "      <td>294</td>\n",
       "      <td>-0.586294</td>\n",
       "      <td>0.190088</td>\n",
       "      <td>1.158856</td>\n",
       "      <td>-0.142448</td>\n",
       "      <td>-0.280257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70260</th>\n",
       "      <td>and I want to congratulate the graphic artist ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>want congratulate graphic artist put entire pr...</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.9421</td>\n",
       "      <td>649</td>\n",
       "      <td>127</td>\n",
       "      <td>-0.073308</td>\n",
       "      <td>0.135404</td>\n",
       "      <td>0.335084</td>\n",
       "      <td>-0.430050</td>\n",
       "      <td>0.216708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Review  is_neagtive_review  \\\n",
       "165256  Having tried a couple of other brands of glute...                   1   \n",
       "231465  My cat loves these treats. If ever I can't fin...                   1   \n",
       "427827  A little less than I expected.  It tends to ha...                   0   \n",
       "433954  First there was Frosted Mini-Wheats, in origin...                   0   \n",
       "70260   and I want to congratulate the graphic artist ...                   1   \n",
       "\n",
       "        predicted_review                                       review_clean  \\\n",
       "165256                 0  tried couple brand gluten-free sandwich cooky ...   \n",
       "231465                 0  cat love treat ever can't find house pop top b...   \n",
       "427827                 0  little less expected tends muddy taste expect ...   \n",
       "433954                 0  first frost mini-wheats original size frost mi...   \n",
       "70260                  0  want congratulate graphic artist put entire pr...   \n",
       "\n",
       "          neg    neu    pos  compound  nb_chars  nb_words  doc2vec_vector_0  \\\n",
       "165256  0.000  0.768  0.232    0.9684       485        87          0.255631   \n",
       "231465  0.089  0.766  0.144    0.7920       490        99         -0.078403   \n",
       "427827  0.000  0.880  0.120    0.4588       136        29          0.120996   \n",
       "433954  0.009  0.827  0.163    0.9923      1631       294         -0.586294   \n",
       "70260   0.089  0.719  0.191    0.9421       649       127         -0.073308   \n",
       "\n",
       "        doc2vec_vector_1  doc2vec_vector_2  doc2vec_vector_3  doc2vec_vector_4  \n",
       "165256          0.543345          0.316511          0.141402         -0.031420  \n",
       "231465          0.205094          0.073949         -0.509898         -0.354058  \n",
       "427827          0.146771          0.109834         -0.118422          0.041968  \n",
       "433954          0.190088          1.158856         -0.142448         -0.280257  \n",
       "70260           0.135404          0.335084         -0.430050          0.216708  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Amazonreviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fff48629c55ba10d8625ced055d92cfd0621a235",
    "id": "RnRwHXhgBmzt"
   },
   "source": [
    "# Exploratory data analysis\n",
    "\n",
    "\n",
    "In order to have a better understanding of our data, let's explore it a little:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "_uuid": "9b19f8973b5aabec726dfe0a4806e2d88a310b0c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YzjK0pKcBmzt",
    "outputId": "a0ba84fb-e8df-4063-9c91-50978b3ddbfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.956761\n",
      "1    0.043239\n",
      "Name: is_neagtive_review, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    0.78276\n",
       "0    0.21724\n",
       "Name: is_neagtive_review, dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show is_bad_review distribution\n",
    "print(reviews_df[\"is_neagtive_review\"].value_counts())\n",
    "Amazonreviews_df[\"is_neagtive_review\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0d81a097747f9e9d16076cbaa211e7e66ba37a8f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "id": "45lRikUVBmzu",
    "outputId": "7460c6dc-6e0d-4dfe-8585-333e369bcaca"
   },
   "outputs": [],
   "source": [
    "# wordcloud function\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color = 'white',\n",
    "        max_words = 200,\n",
    "        max_font_size = 40, \n",
    "        scale = 3,\n",
    "        random_state = 42\n",
    "    ).generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize = (20, 20))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize = 20)\n",
    "        fig.subplots_adjust(top = 2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "    \n",
    "# print wordcloud\n",
    "show_wordcloud(reviews_df[\"review\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cad7d1ca94c9ce4fff486c0f65450e41790ecd57",
    "id": "uz2aQ25DBmzu"
   },
   "source": [
    "Most of the words are indeed related to the hotels: room, staff, breakfast, etc. Some words are more related to the customer experience with the hotel stay: perfect, loved, expensive, dislike, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d247f98a3ae20c5b1b8227171d12ba375d548132",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "c1eTeQoIBmzv",
    "outputId": "500c64a0-eb29-4ad4-d374-8d7c1a3534e1"
   },
   "outputs": [],
   "source": [
    "# highest positive sentiment reviews (with more than 5 words)\n",
    "reviews_df[reviews_df[\"nb_words\"] >= 5].sort_values(\"pos\", ascending = False)[[\"review\", \"pos\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ca802efbb1bb92214e5addfc400952178970162a",
    "id": "0Qka1q1yBmzv"
   },
   "source": [
    "The most positive reviews indeed correspond to some good feedbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e506946236e230f419c09280cbfbcdfad35ab1a4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "kBtT1_w8Bmzw",
    "outputId": "9bf97608-33e9-4330-9f24-ce3cb42cc583"
   },
   "outputs": [],
   "source": [
    "# lowest negative sentiment reviews (with more than 5 words)\n",
    "reviews_df[reviews_df[\"nb_words\"] >= 5].sort_values(\"neg\", ascending = False)[[\"review\", \"neg\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8064f8760dff31dab1fed7deed313d57655dee94",
    "id": "taUk_m-mBmzw"
   },
   "source": [
    "Some errors can be found among the most negative reviews: Vader sometimes interpret 'no' or 'nothing' as negative words whereas they are sometimes used to say that there were no problems with the hotel. Fortunately, most of the reviews are indeed bad ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "efd086062b8ded2dd18515d66048416e7b979a5c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "YGXh2yeYBmzw",
    "outputId": "373647ae-5120-4485-8d1e-30b794180668"
   },
   "outputs": [],
   "source": [
    "# plot sentiment distribution for positive and negative reviews\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "for x in [0, 1]:\n",
    "    subset = reviews_df[reviews_df['is_neagtive_review'] == x]\n",
    "    \n",
    "    # Draw the density plot\n",
    "    if x == 0:\n",
    "        label = \"Good reviews\"\n",
    "    else:\n",
    "        label = \"Bad reviews\"\n",
    "    sns.distplot(subset['compound'], hist = False, label = label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "09bfd81cce5feeb359cff985e71061c44a9c21de",
    "id": "T56uhKz7Bmzw"
   },
   "source": [
    "The above graph shows the distribution of the reviews sentiments among good reviews and bad ones. We can see that good reviews are for most of them considered as very positive by Vader. On the contrary, bad reviews tend to have lower compound sentiment scores.\n",
    "\n",
    "This shows us that previously computed sentiment features will be very important in our modelling part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6792e4ff851b10ce6e0dc46bb8b81e066a6026e7",
    "id": "M-biEM0EBmzw"
   },
   "source": [
    "# Modelling reviewer_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9269d829bf108611ce8ed17517309a72e2e9e727",
    "id": "zDpAThDyBmzx"
   },
   "source": [
    "We first choose which features we want to use to train our model. Then we split our data into two parts:\n",
    "- one to train our model\n",
    "- one to assess its performances\n",
    "\n",
    "We will next use a Random Forest (RF) classifier for our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "_uuid": "5176528611b2a898af937dedfdb89661d9fd1870",
    "id": "4uD4-rU2Bmzx"
   },
   "outputs": [],
   "source": [
    "# feature selection\n",
    "label = \"is_neagtive_review\"\n",
    "Hotel_ignore_cols = [label, \"review\", \"review_clean\"]\n",
    "Amazon_ignore_cols = [label, \"Review\", \"review_clean\"]\n",
    "Hotel_features = [c for c in reviews_df.columns if c not in Hotel_ignore_cols]\n",
    "Amazon_features = [c for c in Amazonreviews_df.columns if c not in Amazon_ignore_cols]\n",
    "\n",
    "# split the data into train and test\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_trainamazon, X_testamazon, y_trainamazon, y_testamazon = train_test_split(Amazonreviews_df[Amazon_features], Amazonreviews_df[label], test_size = 0.2, random_state = 42)\n",
    "X_trainhotel, X_testhotel, y_trainhotel, y_testhotel = train_test_split(reviews_df[Hotel_features], reviews_df[label], test_size = 0.40, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.22      0.35     11340\n",
      "           1       0.00      0.31      0.00        29\n",
      "\n",
      "    accuracy                           0.22     11369\n",
      "   macro avg       0.50      0.26      0.18     11369\n",
      "weighted avg       0.99      0.22      0.35     11369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harith\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "# Q: fit\n",
    "clf.fit(X_trainhotel ,y_trainhotel)\n",
    "# Q: predict\n",
    "y_pred = clf.predict(X_testamazon)\n",
    "print(classification_report(y_pred, y_testamazon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.18      0.27      8337\n",
      "           1       0.23      0.67      0.34      3032\n",
      "\n",
      "    accuracy                           0.31     11369\n",
      "   macro avg       0.41      0.42      0.31     11369\n",
      "weighted avg       0.50      0.31      0.29     11369\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB \n",
    "#Train using GaussianNB classifier \n",
    "clf = GaussianNB()\n",
    "# Q: fit\n",
    "clf.fit(X_trainhotel, y_trainhotel)\n",
    "# Q: predict\n",
    "y_pred=clf.predict(X_testamazon)\n",
    "print(classification_report(y_pred, y_testamazon))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "_uuid": "59086a25cfed314ea49d54d4ee58df4e194a0342",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "id": "If41_T2NBmzx",
    "outputId": "e719100c-76b7-4fc0-deea-1c5684980416"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.21      0.35     11315\n",
      "           1       0.00      0.17      0.00        54\n",
      "\n",
      "    accuracy                           0.21     11369\n",
      "   macro avg       0.49      0.19      0.18     11369\n",
      "weighted avg       0.98      0.21      0.35     11369\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>compound</td>\n",
       "      <td>0.132160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>doc2vec_vector_0</td>\n",
       "      <td>0.121658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>doc2vec_vector_2</td>\n",
       "      <td>0.099940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>doc2vec_vector_1</td>\n",
       "      <td>0.091996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>doc2vec_vector_3</td>\n",
       "      <td>0.091250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>doc2vec_vector_4</td>\n",
       "      <td>0.090977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nb_chars</td>\n",
       "      <td>0.082761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>0.076208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>0.073111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neu</td>\n",
       "      <td>0.072171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_words</td>\n",
       "      <td>0.067625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>predicted_review</td>\n",
       "      <td>0.000143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             feature  importance\n",
       "4           compound    0.132160\n",
       "7   doc2vec_vector_0    0.121658\n",
       "9   doc2vec_vector_2    0.099940\n",
       "8   doc2vec_vector_1    0.091996\n",
       "10  doc2vec_vector_3    0.091250\n",
       "11  doc2vec_vector_4    0.090977\n",
       "5           nb_chars    0.082761\n",
       "1                neg    0.076208\n",
       "3                pos    0.073111\n",
       "2                neu    0.072171\n",
       "6           nb_words    0.067625\n",
       "0   predicted_review    0.000143"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a random forest classifier\n",
    "rf = RandomForestClassifier(n_estimators = 100, random_state = 42)\n",
    "rf.fit(X_trainhotel, y_trainhotel)\n",
    "y_pred=rf.predict(X_testamazon)\n",
    "print(classification_report(y_pred, y_testamazon))\n",
    "# show feature importance\n",
    "feature_importances_df = pd.DataFrame({\"feature\": Amazon_features, \"importance\": rf.feature_importances_}).sort_values(\"importance\", ascending = False)\n",
    "feature_importances_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d492d226a93da9843ad25c36ceb89645353f1acd",
    "id": "0KAgCM5jBmzy"
   },
   "source": [
    "The most important features are indeed the ones that come from the previous sentiment analysis. The vector representations of the texts also have a lot of importance in our training. Some words appear to have a fairly good importance as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC()\n"
     ]
    }
   ],
   "source": [
    "# train a Support Vector Machine classifier\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC()\n",
    "y_pred = clf.fit(X_trainhotel, y_trainhotel)\n",
    "print(classification_report(y_pred, y_testamazon))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.22      0.35     11330\n",
      "           1       0.00      0.59      0.01        39\n",
      "\n",
      "    accuracy                           0.22     11369\n",
      "   macro avg       0.50      0.40      0.18     11369\n",
      "weighted avg       0.99      0.22      0.35     11369\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Q: fit\n",
    "clf.fit(X_trainhotel ,y_trainhotel)\n",
    "# Q: predict\n",
    "y_pred = clf.predict(X_testamazon)\n",
    "print(classification_report(y_pred, y_testamazon))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sentiment-analysis-with-hotel-reviews.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}